{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=4e-3):\n",
    "        self.params={}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "\n",
    "        scores1 = X.dot(W1) + b1 # forward pass\n",
    "        scores1[scores1<0] == 0 # Relu\n",
    "        scores = scores1.dot(W2) + b2 # foward pass\n",
    "        \n",
    "        # This step is very important!!!!! If not, the exponent of e will be too large\n",
    "        # np.max() will return an array in row, change it to column\n",
    "        scores = scores - np.max(scores, axis=1).reshape(scores.shape[0], -1)\n",
    "        \n",
    "        # tobe used in cross-entropy loss\n",
    "        exp_scores = np.exp(scores)\n",
    "        sum_exp_Scores = np.sum(exp_scores, axis=1) # return array in row\n",
    "        \n",
    "        # cross-entropy loss: -ylogx\n",
    "        loss = np.sum(np.log(sum_exp_scores) - scores[range(scores.shape[0]), y]) # sum over axis 0\n",
    "        loss = loss / X.shape[0] # loss should be divided by the instance number N\n",
    "        loss += 0.5 * reg * (np.sum(W1*W1) + np.sum(W2*W2)) # L2 regularization\n",
    "        \n",
    "        # Backpropagation\n",
    "        correct_martix = np.zeros_like(scores)\n",
    "        correct_matrix[range(scores.shape[0]), y] = 1\n",
    "        # see mathematics below\n",
    "        d_loss_scores = exp_scores/sum_exp_scores.reshape(-1, 1) - correct_matrix # same shape with scores: (n, 10)\n",
    "        \n",
    "        # abbr. dx: d_loss_x\n",
    "        dW1 = np.zeros_like(W1) # Assume (3000, 100)\n",
    "        dW2 = np.zeros_like(W2) # Assume (100, 10)\n",
    "        db1 = np.zeros_like(b1) # Assume (100,)\n",
    "        db2 = np.zeros_like(b2) # Assume (10,)\n",
    "        \n",
    "        # Assume X: (n, 3000) ==> scores1 = X.dot(W1) (n, 100) ==> scores = scores1.dot(W2) (n, 10)\n",
    "        \n",
    "        # dL/dW2 = dL/dScores(d_loss_scores) * dScores/W2(scores1)\n",
    "        dW2 += scores1.T.dot(d_loss_scores) # (100, n) * (n, 10) = (100, 10) shape is a good indicator\n",
    "        dW2 /= X.shape[0]\n",
    "        dW2 += reg * dW2\n",
    "        \n",
    "        # todo: matrix derivatives\n",
    "        db2 += np.sum(d_loss_scores, axis=0)\n",
    "        db2 /= X.shape[0]\n",
    "        \n",
    "        # Now the hidden_layer - output part backpropagation is done\n",
    "        # Next is input_layer - hidden_layer part backpropagation\n",
    "        \n",
    "        # dL/dW1(3000, 100) = dL/dScores(d_loss_scores)(n, 10) \n",
    "        #                     * dScores/dScores1(W2)(100, 10) \n",
    "        #                     * dScores1/dW1(X)(n, 3000)\n",
    "        # We should first solve for dL/dscores1 (n, 100):\n",
    "        d_loss_scores1 = d_loss_scores.dot(W2.T)\n",
    "        d_loss_scores1[scores1==0] = 0 # We used Relu activation, where scores1==0 contributes no gradient\n",
    "        \n",
    "        dW1 += X.T.dot(d_loss_scores1)\n",
    "        dW1 /= X.shape[0]\n",
    "        dW1 += reg * dW1\n",
    "        \n",
    "        db1 += np.sum(d_loss_scores1, axis=0)\n",
    "        db1 /= X.shape[0]\n",
    "        \n",
    "        grad = dict()\n",
    "        grads['W1'] = dW1\n",
    "        grads['W2'] = dW2\n",
    "        grads['b1'] = db1\n",
    "        grads['b2'] = db2\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "              reg=5e-6, num_iters=10000, batch_size=200, verbose=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross-entropy loss**:\n",
    "\n",
    "$loss = -ylog(\\hat{y}) = \\sum_{i}{y_i}(\\log\\sum_{j}e^{\\hat{y_j}} - \\hat{y_i}) = $\n",
    "\n",
    "where $i, j \\in [0, 9]$\n",
    "\n",
    "Only for the right label $y_k = 1$, else $y = 0$, so:\n",
    "\n",
    "$loss = \\log\\sum_{j}e^{\\hat{y_j}} - \\hat{y_k}$\n",
    "\n",
    "**d_loss_scores:**\n",
    "\n",
    "For $\\frac{\\partial{loss}}{\\partial{\\hat{y_i}}}$, two possibilities exist:\n",
    "\n",
    "if $i = k$: $= \\frac{\\partial{\\log\\sum_{j}e^{\\hat{y_j}}}}{\\partial{\\hat{y_i}}}\n",
    "- 1 = \\frac{e^{\\hat{y_i}}}{\\sum_{j}e^{\\hat{y_j}}} - 1$\n",
    "\n",
    "\n",
    "\n",
    "if $i \\ne k:$ = $\\frac{e^{\\hat{y_i}}}{\\sum_{j}e^{\\hat{y_j}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
