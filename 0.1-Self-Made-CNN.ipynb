{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_indices(height_or_width, filter_size, stride, ith_element):\n",
    "    n = 0\n",
    "    result = []\n",
    "    while True:\n",
    "        min = stride * n\n",
    "        max = filter_size - 1 + stride * n\n",
    "        if max > height_or_width - 1:\n",
    "            break\n",
    "        if min <= ith_element <= max:\n",
    "            result.append(ith_element-stride*n)\n",
    "        else:\n",
    "            if result:\n",
    "                break\n",
    "        n += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet:\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the following architecture:\n",
    "\n",
    "    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_dim=(3, 32, 32), filter_num=32, filter_size=7, \n",
    "                 pad=0, stride=1, pool_size=2, hidden_dim=100, num_classes=10, \n",
    "                 weight_scale=1e-3, reg=0.0, dtype=np.float32):\n",
    "        self.params = {}\n",
    "        self.filter_num = filter_num\n",
    "        self.filter_size = filter_size\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        self.pool_size = pool_size\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        W1 = np.random.randn(filter_num, input_dim[0], filter_size, filter_size) * weight_scale\n",
    "        b1 = np.zeros((filter_num,))\n",
    "        \n",
    "        feature_map_h = (input_dim[1] - filter_size + 2*pad)/stride + 1 # H2 = (H1-F+2*P)/S+1\n",
    "        feature_map_w = (input_dim[2] - filter_size + 2*pad)/stride + 1 # W2 = (W1-F+2*P)/S+1\n",
    "        \n",
    "        if not feature_map_h//1 == feature_map_h or not feature_map_w//1 == feature_map_w: # W2, H2 should be int\n",
    "            raise ValueError('feature map width and height must be int, adjust filter size, padding or stride.')\n",
    "        \n",
    "        self.feature_map_h, self.feature_map_w = int(feature_map_h), int(feature_map_w)\n",
    "        \n",
    "        self.pooled_feature_map_h = math.ceil(feature_map_h/pool_size)\n",
    "        self.pooled_feature_map_w = math.ceil(feature_map_h/pool_size)\n",
    "        \n",
    "        W2 = np.random.randn(self.pooled_feature_map_h * self.pooled_feature_map_w * filter_num, hidden_dim) \\\n",
    "             * weight_scale\n",
    "        b2 = np.zeros((hidden_dim,))\n",
    "        \n",
    "        W3 = np.random.randn(hidden_dim, num_classes) * weight_scale\n",
    "        b3 = np.zeros((num_classes,))\n",
    "        \n",
    "        self.params['W1'], self.params['b1'] = W1, b1\n",
    "        self.params['W2'], self.params['b2'] = W2, b2\n",
    "        self.params['W3'], self.params['b3'] = W3, b3\n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "        X = np.pad(X, pad_width=((0,), (0,), (self.pad,), (self.pad,)), mode='constant', constant_values=(0,))\n",
    "        feature_map = np.zeros((X.shape[0], self.filter_num, self.feature_map_h, \n",
    "                                                             self.feature_map_w)) # with shape (H2, W2)\n",
    "    \n",
    "        # forward pass\n",
    "\n",
    "        # conv layer:\n",
    "        # todo: is there more efficient way?\n",
    "        for i in range(self.feature_map_h):\n",
    "            for j in range(self.feature_map_w):\n",
    "                for k in range(self.filter_num):\n",
    "                    feature_map[:,k,i,j] = np.sum(X[:, :, i*self.stride: i*self.stride + self.filter_size, \n",
    "                                                         j*self.stride: j*self.stride + self.filter_size]\n",
    "                                                  * W1[k,:,:,:], axis=(1, 2, 3))\n",
    "        feature_map += b1[None, :, None, None]\n",
    "        \n",
    "        # relu:\n",
    "        feature_map[feature_map < 0] = 0\n",
    "        \n",
    "        # max pooling\n",
    "        pooled_feature_map = np.zeros((X.shape[0], self.filter_num, self.pooled_feature_map_h, \n",
    "                                                                    self.pooled_feature_map_w))\n",
    "        \n",
    "        patch = np.ones_like(feature_map)\n",
    "        for i in range(self.pooled_feature_map_h):\n",
    "            for j in range(self.pooled_feature_map_w):\n",
    "                # max value of all 2*2 grid (pooling size) through batch axis and channel axis\n",
    "                local_max = np.max(feature_map[:, :, i*self.pool_size: (i+1)*self.pool_size, \n",
    "                                                     j*self.pool_size: (j+1)*self.pool_size], axis=(2, 3))\n",
    "                pooled_feature_map[:, :, i, j] = local_max\n",
    "                # patch has same shape with feature map but stores max values of all 2*2 grid in all 4 grids\n",
    "                patch[:, :, i*self.pool_size: (i+1)*self.pool_size, j*self.pool_size: (j+1)*self.pool_size] \\\n",
    "                    *= local_max[:, :, None, None]\n",
    "        \n",
    "        # indices of max values of all 2*2 grid\n",
    "        # to be used in d_loss_feature_map\n",
    "        indices = np.argwhere(patch==feature_map)\n",
    "                \n",
    "        # first fully connected layer\n",
    "        scores1 = pooled_feature_map.reshape((X.shape[0], -1)) # Assumed shape: (n, 1000)\n",
    "        scores2 = scores1.dot(W2) + b2 # (n, 100)\n",
    "        \n",
    "        # relu\n",
    "        scores2[scores2 < 0] = 0\n",
    "        \n",
    "        # second fully connected layer\n",
    "        scores = scores2.dot(W3) + b3\n",
    "        \n",
    "        # softmax\n",
    "        exp_scores = np.exp(scores)\n",
    "        sum_exp_scores = np.sum(exp_scores, axis=1)\n",
    "        \n",
    "        loss = np.sum(np.log(sum_exp_scores) - scores[range(scores.shape[0]), y])\n",
    "        loss = loss/X.shape[0]\n",
    "        loss += 0.5 * reg* (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\n",
    "        \n",
    "        # Backpropagation\n",
    "        correct_matrix = np.zeros_like(scores)\n",
    "        correct_matrix[range(scores.shape[0]), y] = 1\n",
    "        d_loss_scores = exp_scores / sum_exp_scores.reshape(-1,1) - correct_matrix # (n, 10)\n",
    "        \n",
    "                                #  Assumed shape for bp deduction\n",
    "        dW3 = np.zeros_like(W3) # (100, 10)\n",
    "        db3 = np.zeros_like(b3) # (10,)\n",
    "        dW2 = np.zeros_like(W2) # (1000, 100)\n",
    "        db2 = np.zeros_like(b2) # (1000,)\n",
    "        dW1 = np.zeros_like(W1) # (64, 3, 2, 2)\n",
    "        db1 = np.zeros_like(b1) # (64,)\n",
    "        dX = np.zeros_like(X)   # (n, 3, 28, 28)\n",
    "        \n",
    "        dW3 += scores2.T.dot(d_loss_scores)\n",
    "        dW3 /= X.shape[0]\n",
    "        dW3 += reg * W3\n",
    "        \n",
    "        db3 += np.sum(d_loss_scores, axis=0)\n",
    "        db3 /= X.shape[0]\n",
    "        \n",
    "        d_loss_scores2 = d_loss_scores.dot(W3.T)\n",
    "        d_loss_scores2[scores2==0] = 0\n",
    "        \n",
    "        dW2 += scores1.T.dot(d_loss_scores2)\n",
    "        dW2 /= X.shape[0]\n",
    "        dW2 += reg * W2\n",
    "        \n",
    "        db2 += np.sum(d_loss_scores2, axis=0)\n",
    "        db2 /= X.shape[0]\n",
    "        \n",
    "        d_loss_scores1 = d_loss_scores2.dot(W2.T)\n",
    "        d_loss_pooled_feature_map = d_loss_scores1.reshape(pooled_feature_map.shape) # recover its original shape\n",
    "        \n",
    "        # Pooling layer bp, only max elements gets gradients, we've stored max elements' indices\n",
    "        d_loss_feature_map = np.zeros_like(feature_map) # (n, 64, pooled_size, pooled_size)\n",
    "        d_loss_feature_map[indices] = d_loss_pooled_feature_map.reshape(X.shape[0],-1) # flatten all non-batch axis\n",
    "        d_loss_feature_map[feature_map==0] = 0 # because of relu\n",
    "        \n",
    "        # conv layer bp, only consider 2D circumstance, intuition:\n",
    "        # d_loss_Wij = Wij_scanned_X * d_loss_feature_map\n",
    "        for i in range(self.filter_size):\n",
    "            for j in range(self.filter_size):\n",
    "                for k in range(X.shape[1]):\n",
    "                    # shape: (n, 1, w, h)\n",
    "                    scanned_X = X[:, k, i:X.shape[0]-self.filter_size+1+i:self.stride, \n",
    "                                        j:X.shape[1]-self.filter_size+1+j:self.stride]\n",
    "                    # broadcasting: (n, 1, w, h) * (64, w, h) = (n, 64, w, h)\n",
    "                    dW1[:, k, i, j] = np.sum(scanned_X * d_loss_feature_map, axis=(0, 2, 3)) \n",
    "        \n",
    "        dW1 /= X.shape[0]\n",
    "        dW1 += reg * W1\n",
    "        \n",
    "        db1 += np.sum(d_loss_feature_map, axis=(0, 2, 3))\n",
    "        db1 /= X.shape[0]\n",
    "        \n",
    "        # d_loss_X\n",
    "        # When we have more than one conv layers we need this, if only one conv layer, not needed.\n",
    "        for i in range(X.shape[2]):\n",
    "            for j in range(X.shape[3]):\n",
    "                # Ws scanned through Xij, we get their x,y indices here\n",
    "                W_indices_x = get_w_indices(X.shape[2], self.filter_size, self.stride, i)\n",
    "                W_indices_y = get_w_indices(X.shape[3], self.filter_size, self.stride, j)\n",
    "                \n",
    "                # indices of Ws and ys that are related to Xij\n",
    "                W_indices = np.array([[x, y] for x in W_indices_x for y in W_indices_y])\n",
    "                y_indices = (np.array([i, j]) - W_indices)/self.stride\n",
    "                \n",
    "                # Ws and ys that are related to Xij\n",
    "                scanned_w = W1[:, :, W_indices[:, 0], W_indices[:, 1]] # Assumed: (64, 3, m, n)\n",
    "                scanned_y = feature_map[:, y_indices[:, 0], y_indices[:, 1]] # Assumed (n, 64, m, n)\n",
    "                \n",
    "                # Found that y is mapped with W rotated 180 degree\n",
    "                dX[:, :, i, j] = np.sum(np.rot90(scanned_w, 2) * scanned_y[:, :, None, :, :], axis=(1, 3, 4))\n",
    "                \n",
    "        \n",
    "        grad = dict()\n",
    "        grads['W1'] = dW1\n",
    "        grads['W2'] = dW2\n",
    "        grads['b1'] = db1\n",
    "        grads['b2'] = db2\n",
    "        \n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
